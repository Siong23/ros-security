{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2MYLRimVG0l"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn in c:\\users\\b760m-itx d4 wifi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.7.1)\n",
            "Requirement already satisfied: imbalanced-learn in c:\\users\\b760m-itx d4 wifi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.14.0)\n",
            "Requirement already satisfied: optuna in c:\\users\\b760m-itx d4 wifi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.5.0)\n",
            "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\b760m-itx d4 wifi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (2.3.2)\n",
            "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\b760m-itx d4 wifi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\b760m-itx d4 wifi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\b760m-itx d4 wifi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in c:\\users\\b760m-itx d4 wifi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from optuna) (1.16.5)\n",
            "Requirement already satisfied: colorlog in c:\\users\\b760m-itx d4 wifi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from optuna) (6.9.0)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\b760m-itx d4 wifi\\appdata\\roaming\\python\\python312\\site-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in c:\\users\\b760m-itx d4 wifi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from optuna) (2.0.43)\n",
            "Requirement already satisfied: tqdm in c:\\users\\b760m-itx d4 wifi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in c:\\users\\b760m-itx d4 wifi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in c:\\users\\b760m-itx d4 wifi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in c:\\users\\b760m-itx d4 wifi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
            "Requirement already satisfied: greenlet>=1 in c:\\users\\b760m-itx d4 wifi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
            "Requirement already satisfied: colorama in c:\\users\\b760m-itx d4 wifi\\appdata\\roaming\\python\\python312\\site-packages (from colorlog->optuna) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\b760m-itx d4 wifi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 25.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "!\"{sys.executable}\" -m pip install \\\n",
        "    pandas numpy scikit-learn imbalanced-learn optuna \\\n",
        "    matplotlib seaborn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pandas\n",
            "  Downloading pandas-2.3.2-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: numpy in c:\\users\\b760m-itx d4 wifi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\b760m-itx d4 wifi\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
            "Collecting pytz>=2020.1 (from pandas)\n",
            "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas)\n",
            "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\b760m-itx d4 wifi\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading pandas-2.3.2-cp312-cp312-win_amd64.whl (11.0 MB)\n",
            "   ---------------------------------------- 0.0/11.0 MB ? eta -:--:--\n",
            "    --------------------------------------- 0.3/11.0 MB 5.2 MB/s eta 0:00:03\n",
            "   ----- ---------------------------------- 1.4/11.0 MB 15.0 MB/s eta 0:00:01\n",
            "   --------- ------------------------------ 2.7/11.0 MB 18.9 MB/s eta 0:00:01\n",
            "   ------------ --------------------------- 3.5/11.0 MB 18.4 MB/s eta 0:00:01\n",
            "   ---------------- ----------------------- 4.7/11.0 MB 19.9 MB/s eta 0:00:01\n",
            "   --------------------- ------------------ 6.0/11.0 MB 21.2 MB/s eta 0:00:01\n",
            "   --------------------------- ------------ 7.6/11.0 MB 23.1 MB/s eta 0:00:01\n",
            "   -------------------------------- ------- 8.9/11.0 MB 23.8 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 10.4/11.0 MB 26.2 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 11.0/11.0 MB 27.3 MB/s eta 0:00:00\n",
            "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
            "   ---------------------------------------- 0.0/509.2 kB ? eta -:--:--\n",
            "   -- ------------------------------------- 30.7/509.2 kB ? eta -:--:--\n",
            "   -- ------------------------------------- 30.7/509.2 kB ? eta -:--:--\n",
            "   -- ------------------------------------- 30.7/509.2 kB ? eta -:--:--\n",
            "   ------- ------------------------------- 92.2/509.2 kB 521.8 kB/s eta 0:00:01\n",
            "   ----------- -------------------------- 153.6/509.2 kB 762.6 kB/s eta 0:00:01\n",
            "   ------------------- ------------------ 256.0/509.2 kB 983.0 kB/s eta 0:00:01\n",
            "   -------------------------------- ------- 419.8/509.2 kB 1.3 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 509.2/509.2 kB 1.5 MB/s eta 0:00:00\n",
            "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
            "   ---------------------------------------- 0.0/347.8 kB ? eta -:--:--\n",
            "   --------------------------------------- 347.8/347.8 kB 22.5 MB/s eta 0:00:00\n",
            "Installing collected packages: pytz, tzdata, pandas\n",
            "Successfully installed pandas-2.3.2 pytz-2025.2 tzdata-2025.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 25.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "!\"{sys.executable}\" -m pip install pandas numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "u-X_FHX6VG0n"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'matplotlib'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mimblearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mover_sampling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SMOTE\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mimblearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01munder_sampling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RandomUnderSampler\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'matplotlib'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.tree import plot_tree\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.feature_selection import mutual_info_classif, SelectKBest\n",
        "import optuna\n",
        "from sklearn.decomposition import PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6TztZ-m9VG0n"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "file_path = \"/content/NavBot25.csv\"  #replace here /content/NavBot25.csv\n",
        "data = pd.read_csv(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oj39_1hMVG0o"
      },
      "outputs": [],
      "source": [
        "# Display dataset info\n",
        "print(data.info())\n",
        "print(data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLl7FY1WVG0o"
      },
      "outputs": [],
      "source": [
        "print(data['Label'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KY9A6I4mVG0o"
      },
      "outputs": [],
      "source": [
        "# Define attack type mapping\n",
        "attack_mapping = {\n",
        "    \"Normal\": 0,\n",
        "    \"DoS Attack\": 1,\n",
        "    \"UnauthSub Attack\": 2,\n",
        "    \"SSH Bruteforce\": 3,\n",
        "    \"Pubflood\": 4,\n",
        "    \"Subflood\": 5,\n",
        "    \"Reverse Shell\": 6,\n",
        "    \"Port Scanning Attack\": 7\n",
        "}\n",
        "\n",
        "# Convert attack type names to numeric labels\n",
        "data[\"Label\"] = data[\"Label\"].map(attack_mapping)\n",
        "\n",
        "# Drop rows with unmatched labels (if any)\n",
        "data = data.dropna(subset=[\"Label\"])\n",
        "\n",
        "# Ensure labels are integers\n",
        "data[\"Label\"] = data[\"Label\"].astype(int)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKpbBSEqVG0o"
      },
      "outputs": [],
      "source": [
        "# Drop unnecessary columns\n",
        "columns_to_drop = ['Flow ID', 'Src IP', 'Dst IP', 'Protocol', 'Timestamp']\n",
        "data = data.drop(columns=columns_to_drop, errors='ignore')\n",
        "\n",
        "# Check if any column is non-numeric\n",
        "non_numeric_columns = data.select_dtypes(exclude=['number']).columns\n",
        "print(\"Non-numeric columns:\", non_numeric_columns)\n",
        "\n",
        "# Handle missing values for numeric columns only\n",
        "numeric_columns = data.select_dtypes(include=['number']).columns\n",
        "data[numeric_columns] = data[numeric_columns].fillna(data[numeric_columns].mean())\n",
        "\n",
        "# Check the dataset again\n",
        "print(data.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RIGL3wzMVG0p"
      },
      "outputs": [],
      "source": [
        "# Split into features (X) and target (y)\n",
        "X = data.drop('Label', axis=1)  # Features\n",
        "y = data['Label']  # Target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9Xg4-4CVG0p"
      },
      "outputs": [],
      "source": [
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Training set size:\", X_train.shape)\n",
        "print(\"Testing set size:\", X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChvJX5QxVG0p"
      },
      "outputs": [],
      "source": [
        "# Check for NaN values\n",
        "print(\"NaN values in X_train:\", np.isnan(X_train).sum().sum())\n",
        "print(\"NaN values in X_test:\", np.isnan(X_test).sum().sum())\n",
        "\n",
        "# Check for infinity values\n",
        "print(\"Infinity values in X_train:\", np.isinf(X_train).sum().sum())\n",
        "print(\"Infinity values in X_test:\", np.isinf(X_test).sum().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pNVBYrNVG0p"
      },
      "outputs": [],
      "source": [
        "# Replace NaN and infinity with the mean of the column\n",
        "X_train = X_train.replace([np.inf, -np.inf], np.nan)\n",
        "X_test = X_test.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "# Fill NaN with column mean\n",
        "X_train = X_train.fillna(X_train.mean())\n",
        "X_test = X_test.fillna(X_test.mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5sXnDicSVG0q"
      },
      "outputs": [],
      "source": [
        "# Print original class distribution\n",
        "print(\"Original class distribution:\")\n",
        "print(y_train.value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4RhVrAuBVG0q"
      },
      "outputs": [],
      "source": [
        "# Plot class distribution before SMOTE\n",
        "plt.figure(figsize=(8, 6))\n",
        "y_train.value_counts().sort_index().plot(kind='bar', color='skyblue')\n",
        "plt.xlabel(\"Class Label\")\n",
        "plt.ylabel(\"Number of Samples\")\n",
        "plt.title(\"Class Distribution Before SMOTE\")\n",
        "plt.xticks([0, 1, 2, 3, 4, 5, 6, 7], ['Normal', 'DoS Attack', 'UnauthSub Attack', 'SSH Bruteforce', 'UnauthPub Attack', 'Subflood', 'Reverse Shell', 'Port Scanning Attack'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAhWc1afVG0q"
      },
      "outputs": [],
      "source": [
        "# Apply SMOTE to balance the training set\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Check the class distribution after SMOTE\n",
        "print(\"Class distribution after SMOTE:\")\n",
        "print(y_train_balanced.value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0WGduefbVG0q"
      },
      "outputs": [],
      "source": [
        "# Plot class distribution after SMOTE\n",
        "plt.figure(figsize=(8, 6))\n",
        "y_train_balanced.value_counts().sort_index().plot(kind='bar', color='skyblue')\n",
        "plt.xlabel(\"Class Label\")\n",
        "plt.ylabel(\"Number of Samples\")\n",
        "plt.title(\"Class Distribution After SMOTE\")\n",
        "plt.xticks([0, 1, 2, 3, 4, 5, 6, 7], ['Normal', 'DoS Attack', 'UnauthSub Attack', 'SSH Bruteforce', 'UnauthPub Attack', 'Subflood', 'Reverse Shell', 'Port Scanning Attack'])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9rvvrS4VG0q"
      },
      "outputs": [],
      "source": [
        "def scale_data(X_train, X_test, scale_data=True):\n",
        "    \"\"\"\n",
        "    Scales the data if scale_data is True.\n",
        "    \"\"\"\n",
        "    if scale_data:\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train)\n",
        "        X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
        "\n",
        "        X_test_scaled = scaler.transform(X_test)\n",
        "        X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
        "    else:\n",
        "        X_train_scaled = X_train\n",
        "        X_test_scaled = X_test\n",
        "\n",
        "    return X_train_scaled, X_test_scaled, scaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "noF7_m-oVG0q"
      },
      "outputs": [],
      "source": [
        "def compute_mutual_info(X_train_scaled, y_train):\n",
        "    \"\"\"\n",
        "    Computes mutual information (MI) scores for the features in X_train.\n",
        "    \"\"\"\n",
        "    mi_scores = mutual_info_classif(X_train_scaled, y_train, random_state=42)\n",
        "\n",
        "    # Create MI DataFrame with rank\n",
        "    mi_df = pd.DataFrame({\n",
        "        'Feature': X_train_scaled.columns,\n",
        "        'MI_Score': mi_scores\n",
        "    }).sort_values('MI_Score', ascending=False).reset_index(drop=True)\n",
        "    mi_df['Rank'] = mi_df.index + 1\n",
        "\n",
        "    return mi_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lecu_lk-VG0q"
      },
      "outputs": [],
      "source": [
        "def plot_top_features(mi_df, top_n=20):\n",
        "    \"\"\"\n",
        "    Plots the top_n features based on their Mutual Information (MI) scores.\n",
        "    \"\"\"\n",
        "    top_k = mi_df.head(top_n)\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.barplot(x='MI_Score', y='Feature', data=top_k, palette='viridis')\n",
        "    plt.title(f'Top {top_n} Features by Mutual Information Score', fontsize=16)\n",
        "    plt.xlabel('Mutual Information Score', fontsize=12)\n",
        "    plt.ylabel('Feature', fontsize=12)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# First, scale the data and get the scaler\n",
        "X_train_scaled, X_test_scaled, scaler = scale_data(X_train_balanced, X_test, scale_data=True)\n",
        "\n",
        "# Then, compute mutual information\n",
        "mi_df = compute_mutual_info(X_train_scaled, y_train_balanced)\n",
        "\n",
        "# Print Top 20 Features based on MI Scores\n",
        "print(\"[INFO] Top 20 Features by Mutual Information Scores:\")\n",
        "print(mi_df.head(20))\n",
        "\n",
        "# Plot the top 20 features\n",
        "plot_top_features(mi_df, top_n=20)\n",
        "\n",
        "# Select features based on the mutual information threshold\n",
        "selected_features = mi_df[mi_df['MI_Score'] >= 0.01]['Feature'].tolist()\n",
        "\n",
        "# Now select the features from the original data\n",
        "X_train_selected = X_train_balanced[selected_features]\n",
        "X_test_selected = X_test[selected_features]\n",
        "\n",
        "# Ensure X_train_selected has the same columns as X_test_selected\n",
        "X_train_selected = X_train_balanced[X_test_selected.columns]  # Align training data to test features\n",
        "\n",
        "# Apply scaling to both the train and test data using the same scaler (that was used previously)\n",
        "X_train_scaled = scaler.fit_transform(X_train_selected)  # Fit and transform the train data\n",
        "X_test_scaled = scaler.transform(X_test_selected)  # Transform the test data\n",
        "\n",
        "# Print total number of selected features and their names\n",
        "print(f\"[INFO] Selected {len(selected_features)} Features:\")\n",
        "print(selected_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZ557z59VG0q"
      },
      "outputs": [],
      "source": [
        "pca = PCA()\n",
        "pca.fit(X_train_scaled)  # X_train is your training data\n",
        "explained_variance_ratio = pca.explained_variance_ratio_\n",
        "cumulative_variance_ratio = explained_variance_ratio.cumsum()\n",
        "\n",
        "plt.plot(cumulative_variance_ratio)\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvb-l75aVG0q"
      },
      "outputs": [],
      "source": [
        "# Metrics calculation function\n",
        "def calculate_metrics(y_true, y_pred, conf_matrix):\n",
        "    tn, fp, fn, tp = conf_matrix.ravel()\n",
        "\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred)\n",
        "    recall = recall_score(y_true, y_pred)\n",
        "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "    metrics = {\n",
        "        \"Accuracy\": accuracy,\n",
        "        \"Precision\": precision,\n",
        "        \"Recall\": recall,\n",
        "        \"Specificity\": specificity,\n",
        "        \"F1-score\": f1\n",
        "    }\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def display_metrics(metrics):\n",
        "    for metric_name, metric_value in metrics.items():\n",
        "        print(f\"{metric_name.ljust(12)}: {metric_value * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwg1L5-dVG0r"
      },
      "outputs": [],
      "source": [
        "# Train a base KNN model before hyperparameter tuning\n",
        "print(\"Training base KNN model...\")\n",
        "knn_base = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_base.fit(X_train_scaled, y_train_balanced)\n",
        "knn_base_preds = knn_base.predict(X_test_scaled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZgBltpyVG0r"
      },
      "outputs": [],
      "source": [
        "# Generate and display metrics and confusion matrix for base model\n",
        "print(\"\\nBase KNN Performance:\")\n",
        "print(classification_report(y_test, knn_base_preds))\n",
        "print(f\"Accuracy: {accuracy_score(y_test, knn_base_preds):.4f}\")\n",
        "\n",
        "conf_matrix_base = confusion_matrix(y_test, knn_base_preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gfJNG6SVG0r"
      },
      "outputs": [],
      "source": [
        "# Display confusion matrix as count\n",
        "plt.figure(figsize=(15, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.heatmap(conf_matrix_base, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Normal', 'DoS Attack', 'UnauthSub Attack', 'SSH Bruteforce', 'UnauthPub Attack', 'Subflood', 'Reverse Shell', 'Port Scanning Attack'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix - Count (Base RF)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kztQXW69VG0r",
        "outputId": "ae158f7e-18ac-4943-a42c-5e54edbcfa26"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-09-02 13:17:07,736] A new study created in memory with name: no-name-dbfdaaff-aa64-4cb7-b7f4-b1a82e11034a\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Starting Optuna hyperparameter optimization with 15 trials...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-09-02 13:20:51,576] Trial 0 finished with value: 0.9860182509697043 and parameters: {'n_neighbors': 20, 'weights': 'uniform', 'algorithm': 'ball_tree', 'p': 1}. Best is trial 0 with value: 0.9860182509697043.\n",
            "[I 2025-09-02 13:27:15,893] Trial 1 finished with value: 0.9877110993957376 and parameters: {'n_neighbors': 17, 'weights': 'distance', 'algorithm': 'ball_tree', 'p': 2}. Best is trial 1 with value: 0.9877110993957376.\n",
            "[I 2025-09-02 13:32:57,591] Trial 2 finished with value: 0.9904330688554737 and parameters: {'n_neighbors': 3, 'weights': 'distance', 'algorithm': 'ball_tree', 'p': 2}. Best is trial 2 with value: 0.9904330688554737.\n",
            "[I 2025-09-02 13:52:44,532] Trial 3 finished with value: 0.9889907112589981 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'algorithm': 'brute', 'p': 1}. Best is trial 2 with value: 0.9904330688554737.\n",
            "[I 2025-09-02 13:53:36,255] Trial 4 finished with value: 0.9907028108014997 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'algorithm': 'kd_tree', 'p': 1}. Best is trial 4 with value: 0.9907028108014997.\n",
            "[I 2025-09-02 14:13:21,987] Trial 5 finished with value: 0.9909131586510337 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'algorithm': 'auto', 'p': 1}. Best is trial 5 with value: 0.9909131586510337.\n",
            "[I 2025-09-02 14:15:36,700] Trial 6 finished with value: 0.9895657421717494 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'algorithm': 'auto', 'p': 2}. Best is trial 5 with value: 0.9909131586510337.\n",
            "[I 2025-09-02 14:16:26,013] Trial 7 finished with value: 0.9873433871678844 and parameters: {'n_neighbors': 12, 'weights': 'uniform', 'algorithm': 'kd_tree', 'p': 1}. Best is trial 5 with value: 0.9909131586510337.\n",
            "[I 2025-09-02 14:36:16,689] Trial 8 finished with value: 0.991018326588425 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'algorithm': 'auto', 'p': 1}. Best is trial 8 with value: 0.991018326588425.\n",
            "[I 2025-09-02 14:42:33,618] Trial 9 finished with value: 0.9826114604537017 and parameters: {'n_neighbors': 13, 'weights': 'uniform', 'algorithm': 'ball_tree', 'p': 2}. Best is trial 8 with value: 0.991018326588425.\n"
          ]
        }
      ],
      "source": [
        "# Define the Optuna objective function for hyperparameter tuning\n",
        "def objective(trial):\n",
        "    # Define hyperparameters to optimize\n",
        "    n_neighbors = trial.suggest_int('n_neighbors', 1, 30)\n",
        "    weights = trial.suggest_categorical('weights', ['uniform', 'distance'])\n",
        "    algorithm = trial.suggest_categorical('algorithm', ['auto', 'ball_tree', 'kd_tree', 'brute'])\n",
        "    p = trial.suggest_int('p', 1, 2)  # p=1 for Manhattan, p=2 for Euclidean\n",
        "\n",
        "    # Create and train the KNN model\n",
        "    knn = KNeighborsClassifier(\n",
        "        n_neighbors=n_neighbors,\n",
        "        weights=weights,\n",
        "        algorithm=algorithm,\n",
        "        p=p\n",
        "    )\n",
        "\n",
        "    # Use cross-validation to evaluate model performance\n",
        "    score = 0\n",
        "    try:\n",
        "        knn.fit(X_train_scaled, y_train_balanced)\n",
        "        y_pred = knn.predict(X_test_scaled)\n",
        "        score = f1_score(y_test, y_pred, average='macro') # Optimize for F1 score\n",
        "    except Exception as e:\n",
        "        print(f\"Exception in Optuna trial: {e}\")\n",
        "        return 0.0\n",
        "\n",
        "    return score\n",
        "\n",
        "# Run Optuna optimization\n",
        "print(\"\\nStarting Optuna hyperparameter optimization with 15 trials...\")\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=15)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_params = study.best_params\n",
        "print(f\"\\nBest hyperparameters found: {best_params}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9a4lZ9RoVG0r"
      },
      "outputs": [],
      "source": [
        "# Train a new model with the best hyperparameters\n",
        "print(\"\\nTraining KNN with optimized hyperparameters...\")\n",
        "knn_tuned = KNeighborsClassifier(\n",
        "    n_neighbors=best_params['n_neighbors'],\n",
        "    weights=best_params['weights'],\n",
        "    algorithm=best_params['algorithm'],\n",
        "    p=best_params['p']\n",
        ")\n",
        "knn_tuned.fit(X_train_scaled, y_train_balanced)\n",
        "knn_tuned_preds = knn_tuned.predict(X_test_scaled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmuSoenJVG0r"
      },
      "outputs": [],
      "source": [
        "# Get the confusion matrix\n",
        "cm = confusion_matrix(y_test, knn_tuned_preds)\n",
        "\n",
        "# Create a figure with two subplots\n",
        "plt.figure(figsize=(16, 7))\n",
        "\n",
        "# Plot 1: Confusion Matrix (Counts)\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "plt.title('Confusion Matrix (Counts)', fontsize=15)\n",
        "plt.xlabel('Predicted Labels', fontsize=12)\n",
        "plt.ylabel('True Labels', fontsize=12)\n",
        "\n",
        "# Plot 2: Confusion Matrix (Percentages)\n",
        "plt.subplot(1, 2, 2)\n",
        "cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
        "sns.heatmap(cm_percent, annot=True, fmt='.1f', cmap='Blues', cbar=False)\n",
        "plt.title('Confusion Matrix (Percentages)', fontsize=15)\n",
        "plt.xlabel('Predicted Labels', fontsize=12)\n",
        "plt.ylabel('True Labels', fontsize=12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print classification report for additional metrics\n",
        "from sklearn.metrics import classification_report\n",
        "print('\\nClassification Report:')\n",
        "print(classification_report(y_test, knn_tuned_preds))\n",
        "\n",
        "# Print overall accuracy\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(f'\\nAccuracy: {accuracy_score(y_test, knn_tuned_preds):.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3yBc1sCVG0r"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Initialize the Random Forest model (you can tune these hyperparameters)\n",
        "rf_model = RandomForestClassifier(n_estimators=100, max_depth=None, random_state=42)\n",
        "\n",
        "# Step 1: Perform 5-fold cross-validation\n",
        "print(\"\\nPerforming 5-Fold Cross Validation...\")\n",
        "cv_scores = cross_val_score(rf_model, X_train_scaled, y_train_balanced, cv=5)\n",
        "\n",
        "# Step 2: Print cross-validation results\n",
        "print(f\"Cross-validation scores for each fold: {cv_scores}\")\n",
        "print(f\"Mean accuracy: {np.mean(cv_scores)}\")\n",
        "print(f\"Standard deviation: {np.std(cv_scores)}\")\n",
        "\n",
        "# Step 3: Visualize the comparison between the folds\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot cross-validation scores\n",
        "plt.plot(range(1, 6), cv_scores, marker='o', label='Validation Accuracy', color='blue', linestyle='-', linewidth=2)\n",
        "\n",
        "# Optional: Compute and plot training accuracy per fold\n",
        "train_scores = [rf_model.fit(X_train_scaled, y_train_balanced).score(X_train_scaled, y_train_balanced) for _ in range(5)]\n",
        "\n",
        "# Plot training accuracy for comparison\n",
        "plt.plot(range(1, 6), train_scores, marker='x', label='Training Accuracy', color='red', linestyle='--', linewidth=2)\n",
        "\n",
        "# Labels and title\n",
        "plt.title('Comparison of Training and Validation Accuracy Across 5-Folds (Random Forest)')\n",
        "plt.xlabel('Fold Number')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Show plot\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 7324106,
          "sourceId": 11670509,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31012,
      "isGpuEnabled": false,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
