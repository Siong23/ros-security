{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee8e1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "print(sys.platform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e7aff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, sklearn, numpy, joblib, imblearn\n",
    "\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"scikit-learn:\", sklearn.__version__)\n",
    "print(\"numpy:\", numpy.__version__)\n",
    "print(\"joblib:\", joblib.__version__)\n",
    "print(\"imbalanced-learn:\", imblearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0d3e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install seaborn optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a0e69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import mutual_info_classif, SelectKBest\n",
    "import optuna\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea427de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "file_path = \"C:\\\\Users\\\\B760M-ITX D4 WIFI\\\\Documents\\\\GitHub\\\\ros-security\\\\dataset\\\\existing\\\\NavBot25.csv\"\n",
    "data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e79973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display dataset info\n",
    "print(data.info())\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af024430",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['Label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3006ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define attack type mapping\n",
    "attack_mapping = {\n",
    "    \"Normal\": 0,\n",
    "    \"DoS Attack\": 1,\n",
    "    \"UnauthSub Attack\": 2,\n",
    "    \"SSH Bruteforce\": 3,\n",
    "    \"Pubflood\": 4,\n",
    "    \"Subflood\": 5,\n",
    "    \"Reverse Shell\": 6,\n",
    "    \"Port Scanning Attack\": 7\n",
    "}\n",
    "\n",
    "# Convert attack type names to numeric labels\n",
    "data[\"Label\"] = data[\"Label\"].map(attack_mapping)\n",
    "\n",
    "# Drop rows with unmatched labels (if any)\n",
    "data = data.dropna(subset=[\"Label\"])\n",
    "\n",
    "# Ensure labels are integers\n",
    "data[\"Label\"] = data[\"Label\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa64f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "columns_to_drop = ['Flow ID', 'Src IP', 'Dst IP', 'Protocol', 'Timestamp']\n",
    "data = data.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "# Check if any column is non-numeric\n",
    "non_numeric_columns = data.select_dtypes(exclude=['number']).columns\n",
    "print(\"Non-numeric columns:\", non_numeric_columns)\n",
    "\n",
    "# Handle missing values for numeric columns only\n",
    "numeric_columns = data.select_dtypes(include=['number']).columns\n",
    "data[numeric_columns] = data[numeric_columns].fillna(data[numeric_columns].mean())\n",
    "\n",
    "# Check the dataset again\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad4021b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into features (X) and target (y)\n",
    "X = data.drop('Label', axis=1)  # Features\n",
    "y = data['Label']  # Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4e23e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training set size:\", X_train.shape)\n",
    "print(\"Testing set size:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdf3474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaN values\n",
    "print(\"NaN values in X_train:\", np.isnan(X_train).sum().sum())\n",
    "print(\"NaN values in X_test:\", np.isnan(X_test).sum().sum())\n",
    "\n",
    "# Check for infinity values\n",
    "print(\"Infinity values in X_train:\", np.isinf(X_train).sum().sum())\n",
    "print(\"Infinity values in X_test:\", np.isinf(X_test).sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d79593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN and infinity with the mean of the column\n",
    "X_train = X_train.replace([np.inf, -np.inf], np.nan)\n",
    "X_test = X_test.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Fill NaN with column mean\n",
    "X_train = X_train.fillna(X_train.mean())\n",
    "X_test = X_test.fillna(X_test.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3807915f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print original class distribution\n",
    "print(\"Original class distribution:\")\n",
    "print(y_train.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98be0a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot class distribution before SMOTE\n",
    "plt.figure(figsize=(8, 6))\n",
    "y_train.value_counts().sort_index().plot(kind='bar', color='skyblue')\n",
    "plt.xlabel(\"Class Label\")\n",
    "plt.ylabel(\"Number of Samples\")\n",
    "plt.title(\"Class Distribution Before SMOTE\")\n",
    "plt.xticks([0, 1, 2, 3, 4, 5, 6, 7], ['Normal', 'DoS Attack', 'UnauthSub Attack', 'SSH Bruteforce', 'UnauthPub Attack', 'Subflood', 'Reverse Shell', 'Port Scanning Attack'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cf6efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE to balance the training set\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Check the class distribution after SMOTE\n",
    "print(\"Class distribution after SMOTE:\")\n",
    "print(y_train_balanced.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f26bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot class distribution after SMOTE\n",
    "plt.figure(figsize=(8, 6))\n",
    "y_train_balanced.value_counts().sort_index().plot(kind='bar', color='skyblue')\n",
    "plt.xlabel(\"Class Label\")\n",
    "plt.ylabel(\"Number of Samples\")\n",
    "plt.title(\"Class Distribution After SMOTE\")\n",
    "plt.xticks([0, 1, 2, 3, 4, 5, 6, 7], ['Normal', 'DoS Attack', 'UnauthSub Attack', 'SSH Bruteforce', 'UnauthPub Attack', 'Subflood', 'Reverse Shell', 'Port Scanning Attack'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab738b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(X_train, X_test, scale_data=True):\n",
    "    \"\"\"\n",
    "    Scales the data if scale_data is True.\n",
    "    \"\"\"\n",
    "    if scale_data:\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "    else:\n",
    "        X_train_scaled = X_train\n",
    "        X_test_scaled = X_test\n",
    "\n",
    "    return X_train_scaled, X_test_scaled, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f690ba23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mutual_info(X_train_scaled, y_train):\n",
    "    \"\"\"\n",
    "    Computes mutual information (MI) scores for the features in X_train.\n",
    "    \"\"\"\n",
    "    mi_scores = mutual_info_classif(X_train_scaled, y_train, random_state=42)\n",
    "\n",
    "    # Create MI DataFrame with rank\n",
    "    mi_df = pd.DataFrame({\n",
    "        'Feature': X_train_scaled.columns,\n",
    "        'MI_Score': mi_scores\n",
    "    }).sort_values('MI_Score', ascending=False).reset_index(drop=True)\n",
    "    mi_df['Rank'] = mi_df.index + 1\n",
    "\n",
    "    return mi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314419d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_features(mi_df, top_n=20):\n",
    "    \"\"\"\n",
    "    Plots the top_n features based on their Mutual Information (MI) scores.\n",
    "    \"\"\"\n",
    "    top_k = mi_df.head(top_n)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.barplot(x='MI_Score', y='Feature', data=top_k, palette='viridis')\n",
    "    plt.title(f'Top {top_n} Features by Mutual Information Score', fontsize=16)\n",
    "    plt.xlabel('Mutual Information Score', fontsize=12)\n",
    "    plt.ylabel('Feature', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# First, scale the data and get the scaler\n",
    "X_train_scaled, X_test_scaled, scaler = scale_data(X_train_balanced, X_test, scale_data=True)\n",
    "\n",
    "# Then, compute mutual information\n",
    "mi_df = compute_mutual_info(X_train_scaled, y_train_balanced)\n",
    "\n",
    "# Print Top 20 Features based on MI Scores\n",
    "print(\"[INFO] Top 20 Features by Mutual Information Scores:\")\n",
    "print(mi_df.head(20))\n",
    "\n",
    "# Plot the top 20 features\n",
    "plot_top_features(mi_df, top_n=20)\n",
    "\n",
    "# Select features based on the mutual information threshold\n",
    "selected_features = mi_df[mi_df['MI_Score'] >= 0.01]['Feature'].tolist()\n",
    "\n",
    "# Now select the features from the original data\n",
    "X_train_selected = X_train_balanced[selected_features]\n",
    "X_test_selected = X_test[selected_features]\n",
    "\n",
    "# Ensure X_train_selected has the same columns as X_test_selected\n",
    "X_train_selected = X_train_balanced[X_test_selected.columns]  # Align training data to test features\n",
    "\n",
    "# Apply scaling to both the train and test data using the same scaler (that was used previously)\n",
    "X_train_scaled = scaler.fit_transform(X_train_selected)  # Fit and transform the train data\n",
    "X_test_scaled = scaler.transform(X_test_selected)  # Transform the test data\n",
    "\n",
    "# Print total number of selected features and their names\n",
    "print(f\"[INFO] Selected {len(selected_features)} Features:\")\n",
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823efa13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA\n",
    "pca = PCA()\n",
    "pca.fit(X_train_scaled)\n",
    "\n",
    "# Get explained variance ratio and cumulative variance ratio\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "cumulative_variance_ratio = explained_variance_ratio.cumsum()\n",
    "\n",
    "# Plot the cumulative explained variance ratio\n",
    "plt.plot(cumulative_variance_ratio, label='Cumulative Explained Variance')\n",
    "\n",
    "# Set a threshold (e.g. 95% variance explained)\n",
    "threshold = 0.95\n",
    "component_count = next((i for i, val in enumerate(cumulative_variance_ratio) if val >= threshold), len(cumulative_variance_ratio)-1)\n",
    "\n",
    "# Plot the vertical line\n",
    "plt.axvline(x=component_count, color='red', linestyle='--', label=f'{component_count+1} Components')\n",
    "\n",
    "# Label the plot\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.legend(loc='best')\n",
    "plt.title(f'Cumulative Explained Variance (Threshold: {threshold*100:.0f}%)')\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09412338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics calculation function\n",
    "def calculate_metrics(y_true, y_pred, conf_matrix):\n",
    "    tn, fp, fn, tp = conf_matrix.ravel()\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "    metrics = {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"Specificity\": specificity,\n",
    "        \"F1-score\": f1\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def display_metrics(metrics):\n",
    "    for metric_name, metric_value in metrics.items():\n",
    "        print(f\"{metric_name.ljust(12)}: {metric_value * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9a3155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a base Decision Tree model before hyperparameter tuning\n",
    "print(\"Training base Decision Tree model...\")\n",
    "dt_base = DecisionTreeClassifier(random_state=42)\n",
    "dt_base.fit(X_train_scaled, y_train_balanced)\n",
    "dt_base_preds = dt_base.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec0d1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and display metrics and confusion matrix for base model\n",
    "print(\"\\nBase Decision Tree Performance:\")\n",
    "print(classification_report(y_test, dt_base_preds, digits=4))\n",
    "print(f\"Accuracy: {accuracy_score(y_test, dt_base_preds):.4f}\")\n",
    "    \n",
    "conf_matrix_base = confusion_matrix(y_test, dt_base_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f68b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display confusion matrix as count\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.heatmap(conf_matrix_base, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Normal', 'DoS Attack', 'UnauthSub Attack', 'SSH Bruteforce', 'UnauthPub Attack', 'Subflood', 'Reverse Shell', 'Port Scanning Attack'], \n",
    "            yticklabels=['Normal', 'DoS Attack', 'UnauthSub Attack', 'SSH Bruteforce', 'UnauthPub Attack', 'Subflood', 'Reverse Shell', 'Port Scanning Attack'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix - Count (Base DT)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a7ad0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Optuna objective function\n",
    "def objective(trial):\n",
    "    max_depth = trial.suggest_int('max_depth', 5, 50)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 20)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n",
    "    criterion = trial.suggest_categorical('criterion', ['gini', 'entropy'])\n",
    "    \n",
    "    model = DecisionTreeClassifier(\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        criterion=criterion,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        model.fit(X_train_scaled, y_train_balanced)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        score = f1_score(y_test, y_pred, average='macro')  # <-- FIXED HERE\n",
    "    except Exception as e:\n",
    "        print(f\"Exception in Optuna trial: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "    return score\n",
    "\n",
    "# Run Optuna optimization\n",
    "print(\"\\nStarting Optuna hyperparameter optimization with 15 trials...\")\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=15)\n",
    "\n",
    "# Best hyperparameters\n",
    "best_params = study.best_params\n",
    "print(f\"\\nBest hyperparameters found: {best_params}\")\n",
    "\n",
    "# Train final model\n",
    "print(\"\\nTraining Decision Tree with optimized hyperparameters...\")\n",
    "dt_tuned = DecisionTreeClassifier(\n",
    "    max_depth=best_params['max_depth'],\n",
    "    min_samples_split=best_params['min_samples_split'],\n",
    "    min_samples_leaf=best_params['min_samples_leaf'],\n",
    "    criterion=best_params['criterion'],\n",
    "    random_state=42\n",
    ")\n",
    "dt_tuned.fit(X_train_scaled, y_train_balanced)\n",
    "dt_tuned_preds = dt_tuned.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f66ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a new model with the best hyperparameters\n",
    "print(\"\\nTraining Decision Tree with optimized hyperparameters...\")\n",
    "dt_tuned = DecisionTreeClassifier(\n",
    "    max_depth=best_params['max_depth'],\n",
    "    min_samples_split=best_params['min_samples_split'],\n",
    "    min_samples_leaf=best_params['min_samples_leaf'],\n",
    "    criterion=best_params['criterion'],\n",
    "    random_state=42\n",
    ")\n",
    "dt_tuned.fit(X_train_scaled, y_train_balanced)\n",
    "dt_tuned_preds = dt_tuned.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ed6dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Get the confusion matrix\n",
    "cm = confusion_matrix(y_test, dt_tuned_preds)\n",
    "\n",
    "# Create a figure with two subplots\n",
    "plt.figure(figsize=(16, 7))\n",
    "\n",
    "# Plot 1: Confusion Matrix (Counts)\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.title('Confusion Matrix (Counts)', fontsize=15)\n",
    "plt.xlabel('Predicted Labels', fontsize=12)\n",
    "plt.ylabel('True Labels', fontsize=12)\n",
    "\n",
    "# Plot 2: Confusion Matrix (Percentages)\n",
    "plt.subplot(1, 2, 2)\n",
    "cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "sns.heatmap(cm_percent, annot=True, fmt='.1f', cmap='Blues', cbar=False)\n",
    "plt.title('Confusion Matrix (Percentages)', fontsize=15)\n",
    "plt.xlabel('Predicted Labels', fontsize=12)\n",
    "plt.ylabel('True Labels', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print classification report for additional metrics\n",
    "from sklearn.metrics import classification_report\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_test, dt_tuned_preds))\n",
    "\n",
    "# Print overall accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(f'\\nAccuracy: {accuracy_score(y_test, dt_tuned_preds):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41f5883",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Step 1: Perform 5-fold cross-validation\n",
    "print(\"\\nPerforming 5-Fold Cross Validation...\")\n",
    "cv_scores = cross_val_score(dt_tuned, X_train_scaled, y_train_balanced, cv=5)\n",
    "\n",
    "# Step 2: Print cross-validation results\n",
    "print(f\"Cross-validation scores for each fold: {cv_scores}\")\n",
    "print(f\"Mean accuracy: {np.mean(cv_scores)}\")\n",
    "print(f\"Standard deviation: {np.std(cv_scores)}\")\n",
    "\n",
    "# Step 3: Visualize the comparison between the folds\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot for cross-validation scores\n",
    "plt.plot(range(1, 6), cv_scores, marker='o', label='Validation Accuracy', color='blue', linestyle='-', linewidth=2)\n",
    "\n",
    "# Optional: If you want to compare training accuracy, you can also plot it (assuming you have training data available)\n",
    "train_scores = [dt_tuned.fit(X_train_scaled, y_train_balanced).score(X_train_scaled, y_train_balanced) for _ in range(5)]  # Mock training accuracy for each fold\n",
    "\n",
    "# Plot training accuracy for comparison (optional)\n",
    "plt.plot(range(1, 6), train_scores, marker='x', label='Training Accuracy', color='red', linestyle='--', linewidth=2)\n",
    "\n",
    "# Labels and title\n",
    "plt.title('Comparison of Training and Validation Accuracy Across 5-Folds (Decision Tree)')\n",
    "plt.xlabel('Fold Number')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c04c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump\n",
    "\n",
    "# Save the trained Decision Tree model\n",
    "dump(dt_base, \"model.joblib\")\n",
    "print(\"Model saved as model.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e641e509",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(scaler, \"scaler.joblib\")\n",
    "print(\"Scaler saved as scaler.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416ef256",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(X.columns)   # or whatever your feature DataFrame is called\n",
    "with open(\"features.txt\", \"w\") as f:\n",
    "    for feat in features:\n",
    "        f.write(feat + \"\\n\")\n",
    "\n",
    "print(\"Features saved as features.txt\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
