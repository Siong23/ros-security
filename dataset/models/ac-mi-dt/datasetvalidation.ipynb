{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc9285e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: c:\\Users\\Asus\\Documents\\GitHub\\ros-security\\dataset\\models\\ac-mi-dt\n",
      "Files in directory: ['acmidt-features.txt', 'acmidt-model.joblib', 'acmidt-scaler.joblib', 'acmidt.ipynb', 'cleaned_for_model.csv', 'datasetvalidation.ipynb']\n",
      "‚úÖ acmidt-model.joblib exists\n",
      "‚úÖ acmidt-scaler.joblib exists\n",
      "‚úÖ acmidt-features.txt exists\n"
     ]
    }
   ],
   "source": [
    "# Quick test to verify model loading\n",
    "import os\n",
    "print(\"Current directory:\", os.getcwd())\n",
    "print(\"Files in directory:\", os.listdir('.'))\n",
    "\n",
    "# Test if model files exist\n",
    "files_to_check = ['acmidt-model.joblib', 'acmidt-scaler.joblib', 'acmidt-features.txt']\n",
    "for file in files_to_check:\n",
    "    if os.path.exists(file):\n",
    "        print(f\"‚úÖ {file} exists\")\n",
    "    else:\n",
    "        print(f\"‚ùå {file} missing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33f2fe0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded successfully: <class 'sklearn.tree._classes.DecisionTreeClassifier'>\n",
      "‚úÖ Scaler loaded successfully: <class 'sklearn.preprocessing._data.StandardScaler'>\n",
      "‚úÖ Features loaded: 78 features\n",
      "First 5 features: ['Src Port', 'Dst Port', 'Flow Duration', 'Tot Fwd Pkts', 'Tot Bwd Pkts']\n"
     ]
    }
   ],
   "source": [
    "# Load model and test basic functionality\n",
    "from joblib import load\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    # Load model, scaler, and features\n",
    "    model = load(\"acmidt-model.joblib\")\n",
    "    scaler = load(\"acmidt-scaler.joblib\")\n",
    "\n",
    "    with open(\"acmidt-features.txt\") as f:\n",
    "        features = [line.strip() for line in f]\n",
    "    \n",
    "    print(f\"‚úÖ Model loaded successfully: {type(model)}\")\n",
    "    print(f\"‚úÖ Scaler loaded successfully: {type(scaler)}\")\n",
    "    print(f\"‚úÖ Features loaded: {len(features)} features\")\n",
    "    print(f\"First 5 features: {features[:5]}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading models: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a91837a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from: C:\\Users\\Asus\\Documents\\GitHub\\ros-security\\dataset\\existing\\NavBot25.csv\n",
      "‚úÖ Dataset loaded: (192213, 84)\n",
      "Dataset columns (first 10): ['Flow ID', 'Src IP', 'Src Port', 'Dst IP', 'Dst Port', 'Protocol', 'Timestamp', 'Flow Duration', 'Tot Fwd Pkts', 'Tot Bwd Pkts']\n",
      "\\nFeature compatibility check:\n",
      "Expected features: 63\n",
      "Dataset columns: 84\n",
      "Missing features: 0 - None\n",
      "Extra columns: 21 - ['Flow ID', 'CWE Flag Count', 'Fwd Seg Size Min', 'Bwd Byts/b Avg', 'ECE Flag Cnt']\n"
     ]
    }
   ],
   "source": [
    "# Test with actual dataset\n",
    "try:\n",
    "    # Load the dataset\n",
    "    dataset_path = \"C:\\\\Users\\\\Asus\\\\Documents\\\\GitHub\\\\ros-security\\\\dataset\\\\existing\\\\NavBot25.csv\"\n",
    "    print(f\"Loading dataset from: {dataset_path}\")\n",
    "    \n",
    "    df = pd.read_csv(dataset_path)\n",
    "    print(f\"‚úÖ Dataset loaded: {df.shape}\")\n",
    "    print(f\"Dataset columns (first 10): {list(df.columns)[:10]}\")\n",
    "    \n",
    "    # Check feature compatibility\n",
    "    expected_features = list(scaler.feature_names_in_)\n",
    "    missing = set(expected_features) - set(df.columns)\n",
    "    extra = set(df.columns) - set(expected_features)\n",
    "    \n",
    "    print(f\"\\\\nFeature compatibility check:\")\n",
    "    print(f\"Expected features: {len(expected_features)}\")\n",
    "    print(f\"Dataset columns: {len(df.columns)}\")\n",
    "    print(f\"Missing features: {len(missing)} - {list(missing)[:5] if missing else 'None'}\")\n",
    "    print(f\"Extra columns: {len(extra)} - {list(extra)[:5] if extra else 'None'}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading dataset: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60dc6eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Starting ROS Security Analysis...\n",
      "Using sample of 1000 rows from 192213 total rows\n",
      "\\nüìä ROS Security Analysis Results:\n",
      "Total samples in dataset: 192,213\n",
      "Samples analyzed: 1,000\n",
      "\\nDetected threats:\n",
      "üõ°Ô∏è Normal: 331 (33.10%)\n",
      "üö® DoS Attack: 159 (15.90%)\n",
      "üö® Port Scanning Attack: 149 (14.90%)\n",
      "üö® Reverse Shell: 148 (14.80%)\n",
      "üö® UnauthSub Attack: 139 (13.90%)\n",
      "üö® SSH Bruteforce: 34 (3.40%)\n",
      "üö® Pubflood: 26 (2.60%)\n",
      "üö® Subflood: 14 (1.40%)\n",
      "\\nüéØ Security Summary:\n",
      "‚ö†Ô∏è Mixed traffic: 33.1% normal, 66.9% attacks\n",
      "\\nüî• Most common attacks:\n",
      "   ‚Ä¢ DoS Attack: 159 cases (15.90%)\n",
      "   ‚Ä¢ Port Scanning Attack: 149 cases (14.90%)\n",
      "   ‚Ä¢ Reverse Shell: 148 cases (14.80%)\n"
     ]
    }
   ],
   "source": [
    "# Complete prediction and analysis\n",
    "from collections import Counter\n",
    "\n",
    "# Define ROS security attack mapping\n",
    "label_map = {\n",
    "    0: \"Normal\",\n",
    "    1: \"DoS Attack\", \n",
    "    2: \"UnauthSub Attack\",\n",
    "    3: \"SSH Bruteforce\",\n",
    "    4: \"Pubflood\",\n",
    "    5: \"Subflood\", \n",
    "    6: \"Reverse Shell\",\n",
    "    7: \"Port Scanning Attack\"\n",
    "}\n",
    "\n",
    "def predict_attacks(csv_file, sample_size=1000):\n",
    "    \"\"\"\n",
    "    Predict attacks on dataset\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load and prepare data\n",
    "        df = pd.read_csv(csv_file)\n",
    "        \n",
    "        # Take a sample for faster processing if dataset is large\n",
    "        if len(df) > sample_size:\n",
    "            df_sample = df.sample(n=sample_size, random_state=42)\n",
    "            print(f\"Using sample of {sample_size} rows from {len(df)} total rows\")\n",
    "        else:\n",
    "            df_sample = df\n",
    "            print(f\"Using all {len(df)} rows\")\n",
    "        \n",
    "        # Keep only model features\n",
    "        expected_features = list(scaler.feature_names_in_)\n",
    "        df_clean = df_sample[expected_features].copy()\n",
    "        \n",
    "        # Clean data\n",
    "        df_clean = df_clean.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "        \n",
    "        # Scale features\n",
    "        X_scaled = scaler.transform(df_clean)\n",
    "        \n",
    "        # Predict\n",
    "        predictions = model.predict(X_scaled)\n",
    "        \n",
    "        # Convert to labels\n",
    "        predicted_labels = [label_map.get(p, f\"Unknown({p})\") for p in predictions]\n",
    "        \n",
    "        return predicted_labels, len(df)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Prediction error: {e}\")\n",
    "        return [], 0\n",
    "\n",
    "# Run prediction\n",
    "print(\"üîç Starting ROS Security Analysis...\")\n",
    "predictions, total_samples = predict_attacks(\"C:\\\\Users\\\\Asus\\\\Documents\\\\GitHub\\\\ros-security\\\\dataset\\\\existing\\\\NavBot25.csv\")\n",
    "\n",
    "if predictions:\n",
    "    # Analyze results\n",
    "    counts = Counter(predictions)\n",
    "    sample_size = len(predictions)\n",
    "    \n",
    "    print(f\"\\\\nüìä ROS Security Analysis Results:\")\n",
    "    print(f\"Total samples in dataset: {total_samples:,}\")\n",
    "    print(f\"Samples analyzed: {sample_size:,}\")\n",
    "    print(f\"\\\\nDetected threats:\")\n",
    "    \n",
    "    for attack_type, count in counts.most_common():\n",
    "        percentage = (count / sample_size) * 100\n",
    "        status = \"üõ°Ô∏è\" if attack_type == \"Normal\" else \"üö®\"\n",
    "        print(f\"{status} {attack_type}: {count:,} ({percentage:.2f}%)\")\n",
    "    \n",
    "    # Security summary\n",
    "    normal_count = counts.get(\"Normal\", 0)\n",
    "    attack_count = sample_size - normal_count\n",
    "    \n",
    "    print(f\"\\\\nüéØ Security Summary:\")\n",
    "    if normal_count == sample_size:\n",
    "        print(\"‚úÖ All traffic is NORMAL - No threats detected!\")\n",
    "    elif normal_count == 0:\n",
    "        print(\"üö® ALL traffic contains ATTACKS - Critical security risk!\")\n",
    "    else:\n",
    "        normal_pct = (normal_count/sample_size)*100\n",
    "        attack_pct = (attack_count/sample_size)*100\n",
    "        print(f\"‚ö†Ô∏è Mixed traffic: {normal_pct:.1f}% normal, {attack_pct:.1f}% attacks\")\n",
    "        \n",
    "        # Show most common attacks\n",
    "        attack_types = {k: v for k, v in counts.items() if k != \"Normal\"}\n",
    "        if attack_types:\n",
    "            print(f\"\\\\nüî• Most common attacks:\")\n",
    "            for attack, count in sorted(attack_types.items(), key=lambda x: x[1], reverse=True)[:3]:\n",
    "                pct = (count/sample_size)*100\n",
    "                print(f\"   ‚Ä¢ {attack}: {count:,} cases ({pct:.2f}%)\")\n",
    "else:\n",
    "    print(\"‚ùå No predictions generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac81825a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features order: ['Src Port', 'Dst Port', 'Flow Duration', 'Tot Fwd Pkts', 'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std', 'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean', 'Bwd Pkt Len Std', 'Flow Byts/s', 'Flow Pkts/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Tot', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Tot', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Len', 'Bwd Header Len', 'Fwd Pkts/s', 'Bwd Pkts/s', 'Pkt Len Min', 'Pkt Len Max', 'Pkt Len Mean', 'Pkt Len Std', 'Pkt Len Var', 'FIN Flag Cnt', 'SYN Flag Cnt', 'RST Flag Cnt', 'PSH Flag Cnt', 'ACK Flag Cnt', 'URG Flag Cnt', 'CWE Flag Count', 'ECE Flag Cnt', 'Down/Up Ratio', 'Pkt Size Avg', 'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'Fwd Byts/b Avg', 'Fwd Pkts/b Avg', 'Fwd Blk Rate Avg', 'Bwd Byts/b Avg', 'Bwd Pkts/b Avg', 'Bwd Blk Rate Avg', 'Subflow Fwd Pkts', 'Subflow Fwd Byts', 'Subflow Bwd Pkts', 'Subflow Bwd Byts', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min']\n"
     ]
    }
   ],
   "source": [
    "from joblib import load\n",
    "import pandas as pd\n",
    "\n",
    "# Load model, scaler, and features\n",
    "model = load(\"acmidt-model.joblib\")\n",
    "scaler = load(\"acmidt-scaler.joblib\")   # skip if you didn't save scaler\n",
    "\n",
    "with open(\"acmidt-features.txt\") as f:\n",
    "    features = [line.strip() for line in f]\n",
    "\n",
    "print(\"Features order:\", features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9f522a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model expects: ['Src Port', 'Dst Port', 'Flow Duration', 'Tot Fwd Pkts', 'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Min', 'Fwd Pkt Len Mean'] ...\n",
      "CSV has: ['Flow ID', 'Src IP', 'Src Port', 'Dst IP', 'Dst Port', 'Protocol', 'Timestamp', 'Flow Duration', 'Tot Fwd Pkts', 'Tot Bwd Pkts'] ...\n",
      "Missing in CSV: set()\n",
      "Extra in CSV: {'Flow ID', 'Dst IP', 'Protocol', 'Label', 'Timestamp', 'Src IP'}\n"
     ]
    }
   ],
   "source": [
    "# Load your new CSV data\n",
    "new_data = pd.read_csv(\"C:\\\\Users\\\\Asus\\\\Documents\\\\GitHub\\\\ros-security\\\\dataset\\\\existing\\\\NavBot25.csv\")\n",
    "\n",
    "# Compare model features vs CSV columns\n",
    "print(\"Model expects:\", features[:10], \"...\")  # first 10 expected features\n",
    "print(\"CSV has:\", list(new_data.columns)[:10], \"...\")  # first 10 columns from CSV\n",
    "\n",
    "missing = set(features) - set(new_data.columns)\n",
    "extra = set(new_data.columns) - set(features)\n",
    "\n",
    "print(\"Missing in CSV:\", missing)\n",
    "print(\"Extra in CSV:\", extra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba6b2439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force reordering of columns to match training order\n",
    "new_data = new_data.reindex(columns=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8c41d24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only model's features (drop all others)\n",
    "new_data = new_data.loc[:, new_data.columns.intersection(features)]\n",
    "\n",
    "# Reorder columns to exactly match training\n",
    "new_data = new_data.reindex(columns=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1a2f8630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected features: ['Src Port', 'Dst Port', 'Flow Duration', 'Tot Fwd Pkts', 'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std', 'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean', 'Bwd Pkt Len Std', 'Flow Byts/s', 'Flow Pkts/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Tot', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Tot', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Len', 'Bwd Header Len', 'Fwd Pkts/s', 'Bwd Pkts/s', 'Pkt Len Min', 'Pkt Len Max', 'Pkt Len Mean', 'Pkt Len Std', 'Pkt Len Var', 'FIN Flag Cnt', 'SYN Flag Cnt', 'RST Flag Cnt', 'PSH Flag Cnt', 'ACK Flag Cnt', 'URG Flag Cnt', 'CWE Flag Count', 'ECE Flag Cnt', 'Down/Up Ratio', 'Pkt Size Avg', 'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'Fwd Byts/b Avg', 'Fwd Pkts/b Avg', 'Fwd Blk Rate Avg', 'Bwd Byts/b Avg', 'Bwd Pkts/b Avg', 'Bwd Blk Rate Avg', 'Subflow Fwd Pkts', 'Subflow Fwd Byts', 'Subflow Bwd Pkts', 'Subflow Bwd Byts', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min']\n",
      "New data columns : ['Src Port', 'Dst Port', 'Flow Duration', 'Tot Fwd Pkts', 'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std', 'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean', 'Bwd Pkt Len Std', 'Flow Byts/s', 'Flow Pkts/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Tot', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Tot', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Len', 'Bwd Header Len', 'Fwd Pkts/s', 'Bwd Pkts/s', 'Pkt Len Min', 'Pkt Len Max', 'Pkt Len Mean', 'Pkt Len Std', 'Pkt Len Var', 'FIN Flag Cnt', 'SYN Flag Cnt', 'RST Flag Cnt', 'PSH Flag Cnt', 'ACK Flag Cnt', 'URG Flag Cnt', 'CWE Flag Count', 'ECE Flag Cnt', 'Down/Up Ratio', 'Pkt Size Avg', 'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'Fwd Byts/b Avg', 'Fwd Pkts/b Avg', 'Fwd Blk Rate Avg', 'Bwd Byts/b Avg', 'Bwd Pkts/b Avg', 'Bwd Blk Rate Avg', 'Subflow Fwd Pkts', 'Subflow Fwd Byts', 'Subflow Bwd Pkts', 'Subflow Bwd Byts', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min']\n",
      "Shape of new_data: (192213, 78)\n"
     ]
    }
   ],
   "source": [
    "print(\"Expected features:\", features)\n",
    "print(\"New data columns :\", list(new_data.columns))\n",
    "print(\"Shape of new_data:\", new_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2110f642",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = new_data[features].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dbe4c280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features: ['Src Port', 'Dst Port', 'Flow Duration', 'Tot Fwd Pkts', 'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std', 'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean', 'Bwd Pkt Len Std', 'Flow Byts/s', 'Flow Pkts/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Tot', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Tot', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Len', 'Bwd Header Len', 'Fwd Pkts/s', 'Bwd Pkts/s', 'Pkt Len Min', 'Pkt Len Max', 'Pkt Len Mean', 'Pkt Len Std', 'Pkt Len Var', 'FIN Flag Cnt', 'SYN Flag Cnt', 'RST Flag Cnt', 'PSH Flag Cnt', 'ACK Flag Cnt', 'URG Flag Cnt', 'CWE Flag Count', 'ECE Flag Cnt', 'Down/Up Ratio', 'Pkt Size Avg', 'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'Fwd Byts/b Avg', 'Fwd Pkts/b Avg', 'Fwd Blk Rate Avg', 'Bwd Byts/b Avg', 'Bwd Pkts/b Avg', 'Bwd Blk Rate Avg', 'Subflow Fwd Pkts', 'Subflow Fwd Byts', 'Subflow Bwd Pkts', 'Subflow Bwd Byts', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min']\n",
      "New data columns: ['Src Port', 'Dst Port', 'Flow Duration', 'Tot Fwd Pkts', 'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std', 'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean', 'Bwd Pkt Len Std', 'Flow Byts/s', 'Flow Pkts/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Tot', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Tot', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Len', 'Bwd Header Len', 'Fwd Pkts/s', 'Bwd Pkts/s', 'Pkt Len Min', 'Pkt Len Max', 'Pkt Len Mean', 'Pkt Len Std', 'Pkt Len Var', 'FIN Flag Cnt', 'SYN Flag Cnt', 'RST Flag Cnt', 'PSH Flag Cnt', 'ACK Flag Cnt', 'URG Flag Cnt', 'CWE Flag Count', 'ECE Flag Cnt', 'Down/Up Ratio', 'Pkt Size Avg', 'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'Fwd Byts/b Avg', 'Fwd Pkts/b Avg', 'Fwd Blk Rate Avg', 'Bwd Byts/b Avg', 'Bwd Pkts/b Avg', 'Bwd Blk Rate Avg', 'Subflow Fwd Pkts', 'Subflow Fwd Byts', 'Subflow Bwd Pkts', 'Subflow Bwd Byts', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min']\n",
      "Same? True\n"
     ]
    }
   ],
   "source": [
    "# Clean up column names (remove hidden spaces etc.)\n",
    "new_data.columns = new_data.columns.str.strip()\n",
    "\n",
    "# Now force subset to ONLY training features\n",
    "new_data = new_data.loc[:, features]\n",
    "\n",
    "# Confirm\n",
    "print(\"Training features:\", features)\n",
    "print(\"New data columns:\", list(new_data.columns))\n",
    "print(\"Same?\", features == list(new_data.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3029f753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler feature names: ['Dst Port', 'Src Port', 'Bwd Header Len', 'Bwd IAT Tot', 'Bwd Pkts/s', 'Bwd IAT Mean', 'Pkt Len Mean', 'Flow Pkts/s', 'Pkt Size Avg', 'Flow Duration', 'Bwd IAT Max', 'Pkt Len Std', 'Pkt Len Var', 'Flow IAT Mean', 'TotLen Bwd Pkts', 'Subflow Bwd Byts', 'Bwd Seg Size Avg', 'Bwd Pkt Len Mean', 'Flow IAT Max', 'Init Bwd Win Byts', 'Pkt Len Max', 'Flow Byts/s', 'Fwd Pkts/s', 'Fwd Header Len', 'Flow IAT Std', 'Bwd IAT Min', 'Subflow Bwd Pkts', 'Tot Bwd Pkts', 'Bwd Pkt Len Max', 'Bwd IAT Std', 'Flow IAT Min', 'Tot Fwd Pkts', 'Subflow Fwd Pkts', 'Bwd Pkt Len Std', 'Bwd Pkt Len Min', 'Fwd IAT Mean', 'Fwd IAT Tot', 'Down/Up Ratio', 'Idle Min', 'Idle Mean', 'Idle Max', 'Fwd IAT Min', 'Active Max', 'Fwd IAT Max', 'Active Mean', 'Active Min', 'SYN Flag Cnt', 'ACK Flag Cnt', 'Fwd IAT Std', 'Fwd Pkt Len Max', 'Fwd Pkt Len Mean', 'Fwd Seg Size Avg', 'Subflow Fwd Byts', 'TotLen Fwd Pkts', 'PSH Flag Cnt', 'Bwd PSH Flags', 'Idle Std', 'Active Std', 'Fwd Act Data Pkts', 'Fwd Pkt Len Min', 'Fwd Pkt Len Std', 'FIN Flag Cnt', 'Pkt Len Min']\n",
      "New data columns    : ['Src Port', 'Dst Port', 'Flow Duration', 'Tot Fwd Pkts', 'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std', 'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean', 'Bwd Pkt Len Std', 'Flow Byts/s', 'Flow Pkts/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Tot', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Tot', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Len', 'Bwd Header Len', 'Fwd Pkts/s', 'Bwd Pkts/s', 'Pkt Len Min', 'Pkt Len Max', 'Pkt Len Mean', 'Pkt Len Std', 'Pkt Len Var', 'FIN Flag Cnt', 'SYN Flag Cnt', 'RST Flag Cnt', 'PSH Flag Cnt', 'ACK Flag Cnt', 'URG Flag Cnt', 'CWE Flag Count', 'ECE Flag Cnt', 'Down/Up Ratio', 'Pkt Size Avg', 'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'Fwd Byts/b Avg', 'Fwd Pkts/b Avg', 'Fwd Blk Rate Avg', 'Bwd Byts/b Avg', 'Bwd Pkts/b Avg', 'Bwd Blk Rate Avg', 'Subflow Fwd Pkts', 'Subflow Fwd Byts', 'Subflow Bwd Pkts', 'Subflow Bwd Byts', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min']\n",
      "Match? False\n"
     ]
    }
   ],
   "source": [
    "print(\"Scaler feature names:\", list(scaler.feature_names_in_))\n",
    "print(\"New data columns    :\", list(new_data.columns))\n",
    "print(\"Match?\", list(scaler.feature_names_in_) == list(new_data.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "61fe4d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = new_data.loc[:, scaler.feature_names_in_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e00d7b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing in CSV: set()\n",
      "Extra in CSV: {'Flow ID', 'CWE Flag Count', 'Fwd Seg Size Min', 'Bwd Byts/b Avg', 'ECE Flag Cnt', 'Protocol', 'Label', 'Init Fwd Win Byts', 'RST Flag Cnt', 'Fwd Pkts/b Avg', 'Bwd Blk Rate Avg', 'Timestamp', 'Fwd Blk Rate Avg', 'Fwd PSH Flags', 'Dst IP', 'Fwd Byts/b Avg', 'Bwd URG Flags', 'URG Flag Cnt', 'Fwd URG Flags', 'Bwd Pkts/b Avg', 'Src IP'}\n",
      "[0 0 0 ... 7 7 7]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from joblib import load\n",
    "\n",
    "# === Load model + scaler ===\n",
    "model = load(\"acmidt-model.joblib\")\n",
    "scaler = load(\"acmidt-scaler.joblib\")\n",
    "\n",
    "# Expected features from scaler\n",
    "expected_features = list(scaler.feature_names_in_)\n",
    "\n",
    "def prepare_and_predict(csv_file):\n",
    "    # 1. Load CSV\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # 2. Keep only required columns (drop timestamps or extras)\n",
    "    missing = set(expected_features) - set(df.columns)\n",
    "    extra = set(df.columns) - set(expected_features)\n",
    "    print(\"Missing in CSV:\", missing)\n",
    "    print(\"Extra in CSV:\", extra)\n",
    "\n",
    "    df = df.loc[:, expected_features]  # keep only what we need, in correct order\n",
    "\n",
    "    # 3. Clean data\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)  # replace inf with NaN\n",
    "    df = df.fillna(0)  # fill NaN with 0 (safe default, you can change strategy)\n",
    "\n",
    "    # 4. Scale\n",
    "    X_scaled = scaler.transform(df)\n",
    "\n",
    "    # 5. Predict\n",
    "    preds = model.predict(X_scaled)\n",
    "    return preds\n",
    "\n",
    "# === Run prediction on your new CSV ===\n",
    "predictions = prepare_and_predict(\"C:\\\\Users\\\\Asus\\\\Documents\\\\GitHub\\\\ros-security\\\\dataset\\\\existing\\\\NavBot25.csv\") # replace data here <-----------------------\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "15b0cfbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned CSV as cleaned_for_model.csv\n"
     ]
    }
   ],
   "source": [
    "def prepare_and_predict(csv_file, save_cleaned=False):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df = df.loc[:, expected_features]\n",
    "    df = df.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "    if save_cleaned:\n",
    "        df.to_csv(\"cleaned_for_model.csv\", index=False)\n",
    "        print(\"Saved cleaned CSV as cleaned_for_model.csv\")\n",
    "\n",
    "    X_scaled = scaler.transform(df)\n",
    "    return model.predict(X_scaled)\n",
    "\n",
    "predictions = prepare_and_predict(\"C:\\\\Users\\\\Asus\\\\Documents\\\\GitHub\\\\ros-security\\\\dataset\\\\existing\\\\NavBot25.csv\", save_cleaned=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c26bb2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_and_predict(csv_file, save_cleaned=False):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df = df.loc[:, expected_features]\n",
    "    df = df.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "    if save_cleaned:\n",
    "        df.to_csv(\"cleaned_for_model.csv\", index=False)\n",
    "        print(\"Saved cleaned CSV as cleaned_for_model.csv\")\n",
    "\n",
    "    X_scaled = scaler.transform(df)\n",
    "    preds = model.predict(X_scaled)\n",
    "\n",
    "    # Convert numeric predictions into labels\n",
    "    decoded_preds = [label_map.get(p, f\"Unknown({p})\") for p in preds]\n",
    "    return decoded_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b4f47732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define label mapping for ROS security attacks\n",
    "label_map = {\n",
    "    0: \"Normal\",\n",
    "    1: \"DoS Attack\",\n",
    "    2: \"UnauthSub Attack\",\n",
    "    3: \"SSH Bruteforce\",\n",
    "    4: \"Pubflood\",\n",
    "    5: \"Subflood\",\n",
    "    6: \"Reverse Shell\",\n",
    "    7: \"Port Scanning Attack\"\n",
    "}\n",
    "\n",
    "def prepare_and_predict(csv_file, save_cleaned=False):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df = df.loc[:, expected_features]\n",
    "    df = df.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "    if save_cleaned:\n",
    "        df.to_csv(\"cleaned_for_model.csv\", index=False)\n",
    "        print(\"Saved cleaned CSV as cleaned_for_model.csv\")\n",
    "\n",
    "    X_scaled = scaler.transform(df)\n",
    "    preds = model.predict(X_scaled)\n",
    "\n",
    "    # Convert numeric predictions into labels\n",
    "    decoded_preds = [label_map.get(p, f\"Unknown({p})\") for p in preds]\n",
    "    return decoded_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "36962999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned CSV as cleaned_for_model.csv\n",
      "['Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal']\n",
      "\n",
      "Summary of predictions:\n",
      "Counter({'Normal': 63013, 'Port Scanning Attack': 29897, 'DoS Attack': 29888, 'Reverse Shell': 29525, 'UnauthSub Attack': 25913, 'SSH Bruteforce': 6136, 'Pubflood': 4715, 'Subflood': 3126})\n",
      "Mix of normal and attack traffic: 63013 normal, 129200 attacks\n"
     ]
    }
   ],
   "source": [
    "predictions = prepare_and_predict(\"C:\\\\Users\\\\Asus\\\\Documents\\\\GitHub\\\\ros-security\\\\dataset\\\\existing\\\\NavBot25.csv\", save_cleaned=True)\n",
    "\n",
    "# Print first 10 results\n",
    "print(predictions[:10])\n",
    "\n",
    "# Count normal vs attack types\n",
    "from collections import Counter\n",
    "counts = Counter(predictions)\n",
    "\n",
    "print(\"\\nSummary of predictions:\")\n",
    "print(counts)\n",
    "\n",
    "if counts[\"Normal\"] == len(predictions):\n",
    "    print(\"‚úÖ All traffic is normal.\")\n",
    "elif counts[\"Normal\"] == 0:\n",
    "    print(\"‚ö†Ô∏è All traffic contains attacks.\")\n",
    "else:\n",
    "    print(f\"Mix of normal and attack traffic: {counts['Normal']} normal, {len(predictions)-counts['Normal']} attacks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "69c55724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned CSV as cleaned_for_model.csv\n",
      "First 10 predictions: ['Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal']\n",
      "\n",
      "Detailed prediction summary:\n",
      "Normal: 63013 (32.78%)\n",
      "SSH Bruteforce: 6136 (3.19%)\n",
      "DoS Attack: 29888 (15.55%)\n",
      "UnauthSub Attack: 25913 (13.48%)\n",
      "Subflood: 3126 (1.63%)\n",
      "Pubflood: 4715 (2.45%)\n",
      "Reverse Shell: 29525 (15.36%)\n",
      "Port Scanning Attack: 29897 (15.55%)\n",
      "\n",
      "Total samples analyzed: 192213\n",
      "‚ö†Ô∏è Mixed traffic detected: 63013 normal (32.8%), 129200 attacks (67.2%)\n"
     ]
    }
   ],
   "source": [
    "predictions = prepare_and_predict(\"C:\\\\Users\\\\Asus\\\\Documents\\\\GitHub\\\\ros-security\\\\dataset\\\\existing\\\\NavBot25.csv\", save_cleaned=True)\n",
    "\n",
    "# Print first 10 results\n",
    "print(\"First 10 predictions:\", predictions[:10])\n",
    "\n",
    "# Count normal vs attack types\n",
    "from collections import Counter\n",
    "counts = Counter(predictions)\n",
    "\n",
    "print(\"\\nDetailed prediction summary:\")\n",
    "for attack_type, count in counts.items():\n",
    "    percentage = (count / len(predictions)) * 100\n",
    "    print(f\"{attack_type}: {count} ({percentage:.2f}%)\")\n",
    "\n",
    "print(f\"\\nTotal samples analyzed: {len(predictions)}\")\n",
    "\n",
    "if counts[\"Normal\"] == len(predictions):\n",
    "    print(\"‚úÖ All traffic is normal - No threats detected.\")\n",
    "elif counts[\"Normal\"] == 0:\n",
    "    print(\"üö® All traffic contains attacks - High risk!\")\n",
    "else:\n",
    "    normal_count = counts['Normal']\n",
    "    attack_count = len(predictions) - normal_count\n",
    "    print(f\"‚ö†Ô∏è Mixed traffic detected: {normal_count} normal ({(normal_count/len(predictions)*100):.1f}%), {attack_count} attacks ({(attack_count/len(predictions)*100):.1f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
