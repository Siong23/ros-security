{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dff378bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T13:09:23.652270Z",
     "iopub.status.busy": "2025-07-11T13:09:23.652039Z",
     "iopub.status.idle": "2025-07-11T13:09:34.076504Z",
     "shell.execute_reply": "2025-07-11T13:09:34.075794Z"
    },
    "papermill": {
     "duration": 10.429421,
     "end_time": "2025-07-11T13:09:34.077851",
     "exception": false,
     "start_time": "2025-07-11T13:09:23.648430",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\r\n",
      "Collecting scikit-learn\r\n",
      "  Downloading scikit_learn-1.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (17 kB)\r\n",
      "Requirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\r\n",
      "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.0->scikit-learn) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.0->scikit-learn) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.0->scikit-learn) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.0->scikit-learn) (2025.1.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.0->scikit-learn) (2022.1.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.0->scikit-learn) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.22.0->scikit-learn) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.22.0->scikit-learn) (2022.1.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.22.0->scikit-learn) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.22.0->scikit-learn) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.22.0->scikit-learn) (2024.2.0)\r\n",
      "Downloading scikit_learn-1.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.9 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: scikit-learn\r\n",
      "  Attempting uninstall: scikit-learn\r\n",
      "    Found existing installation: scikit-learn 1.2.2\r\n",
      "    Uninstalling scikit-learn-1.2.2:\r\n",
      "      Successfully uninstalled scikit-learn-1.2.2\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "category-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.7.0 which is incompatible.\r\n",
      "sklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.7.0 which is incompatible.\r\n",
      "bigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed scikit-learn-1.7.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e9dfc0f",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-11T13:09:34.085302Z",
     "iopub.status.busy": "2025-07-11T13:09:34.085035Z",
     "iopub.status.idle": "2025-07-11T13:09:39.719149Z",
     "shell.execute_reply": "2025-07-11T13:09:39.718142Z"
    },
    "papermill": {
     "duration": 5.638946,
     "end_time": "2025-07-11T13:09:39.720279",
     "exception": true,
     "start_time": "2025-07-11T13:09:34.081333",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_safe_tags' from 'sklearn.utils._tags' (/usr/local/lib/python3.11/dist-packages/sklearn/utils/_tags.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19/2120423913.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover_sampling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/imblearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m# process, as it may not be compiled yet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     from . import (\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mcombine\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mensemble\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/imblearn/ensemble/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_bagging\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBalancedBaggingClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_easy_ensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEasyEnsembleClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_forest\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBalancedRandomForestClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_weight_boosting\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRUSBoostClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/imblearn/ensemble/_easy_ensemble.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_partition_estimators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_param_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInterval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStrOptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tags\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_safe_tags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparse_version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetaestimators\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mavailable_if\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name '_safe_tags' from 'sklearn.utils._tags' (/usr/local/lib/python3.11/dist-packages/sklearn/utils/_tags.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import gc\n",
    "\n",
    "# Deep learning imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Conv1D, Dropout, BatchNormalization, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Optimize for Kaggle\n",
    "tf.config.optimizer.set_jit(True)  # Enable XLA\n",
    "tf.config.experimental.enable_memory_growth = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56255b1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T10:36:01.276350Z",
     "iopub.status.busy": "2025-06-27T10:36:01.275785Z",
     "iopub.status.idle": "2025-06-27T10:36:03.967815Z",
     "shell.execute_reply": "2025-06-27T10:36:03.966941Z",
     "shell.execute_reply.started": "2025-06-27T10:36:01.276324Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Load the dataset\n",
    "print(\"Loading the dataset...\")\n",
    "file_path = \"/kaggle/input/navbot25-v8/NavBot25_V8.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Attack mapping\n",
    "attack_mapping = {\n",
    "    \"Normal\": 0,\n",
    "    \"DoS Attack\": 1,\n",
    "    \"UnauthSub Attack\": 2,\n",
    "    \"SSH Bruteforce\": 3,\n",
    "    \"Pubflood\": 4,\n",
    "    \"Subflood\": 5,\n",
    "    \"Reverse Shell\": 6,\n",
    "    \"Port Scanning Attack\": 7\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f63de46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T10:36:03.970473Z",
     "iopub.status.busy": "2025-06-27T10:36:03.970182Z",
     "iopub.status.idle": "2025-06-27T10:36:05.285269Z",
     "shell.execute_reply": "2025-06-27T10:36:05.284737Z",
     "shell.execute_reply.started": "2025-06-27T10:36:03.970451Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply mapping\n",
    "data[\"Label\"] = data[\"Label\"].map(attack_mapping)\n",
    "data = data.dropna(subset=[\"Label\"])\n",
    "data[\"Label\"] = data[\"Label\"].astype(int)\n",
    "\n",
    "# Drop irrelevant columns\n",
    "columns_to_drop = ['Flow ID', 'Src IP', 'Dst IP', 'Protocol', 'Timestamp']\n",
    "data = data.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "# Fill missing values\n",
    "numeric_columns = data.select_dtypes(include=['number']).columns\n",
    "data[numeric_columns] = data[numeric_columns].fillna(data[numeric_columns].mean())\n",
    "\n",
    "# Split into features and labels\n",
    "X = data.drop('Label', axis=1)\n",
    "y = data['Label']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Handle inf and nan\n",
    "X_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "X_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "X_train.fillna(X_train.mean(), inplace=True)\n",
    "X_test.fillna(X_test.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10fa456",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T10:36:05.286715Z",
     "iopub.status.busy": "2025-06-27T10:36:05.286138Z",
     "iopub.status.idle": "2025-06-27T10:36:15.247195Z",
     "shell.execute_reply": "2025-06-27T10:36:15.246579Z",
     "shell.execute_reply.started": "2025-06-27T10:36:05.286693Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Applying SMOTE...\")\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Standardization\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_balanced)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Reshape for CNN+LSTM (Feature Extraction)\n",
    "timesteps = 1\n",
    "n_features = X_train_scaled.shape[1]\n",
    "X_train_reshaped = X_train_scaled.reshape(X_train_scaled.shape[0], timesteps, n_features)\n",
    "X_test_reshaped = X_test_scaled.reshape(X_test_scaled.shape[0], timesteps, n_features)\n",
    "\n",
    "# One-hot encode labels for CNN+LSTM training\n",
    "num_classes = len(attack_mapping)\n",
    "y_train_onehot = to_categorical(y_train_balanced, num_classes)\n",
    "y_test_onehot = to_categorical(y_test, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d105e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T10:36:15.248118Z",
     "iopub.status.busy": "2025-06-27T10:36:15.247882Z",
     "iopub.status.idle": "2025-06-27T10:42:17.087720Z",
     "shell.execute_reply": "2025-06-27T10:42:17.086967Z",
     "shell.execute_reply.started": "2025-06-27T10:36:15.248095Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LIGHTWEIGHT FEATURE EXTRACTION: Simplified CNN+LSTM\n",
    "# =============================================================================\n",
    "def create_lightweight_feature_extractor(input_shape):\n",
    "    \"\"\"Create lightweight CNN+LSTM model for feature extraction\"\"\"\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # Simplified CNN layers\n",
    "    x = Conv1D(32, kernel_size=1, activation='relu', padding='same')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    # Single LSTM layer\n",
    "    x = LSTM(50, return_sequences=False)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    # Smaller feature extraction layer\n",
    "    features = Dense(32, activation='relu', name='extracted_features')(x)\n",
    "    features = BatchNormalization()(features)\n",
    "    features = Dropout(0.3)(features)\n",
    "    \n",
    "    # Classification head\n",
    "    outputs = Dense(num_classes, activation='softmax')(features)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    feature_extractor = Model(inputs, features)\n",
    "    \n",
    "    return model, feature_extractor\n",
    "\n",
    "print(\"Creating lightweight CNN+LSTM feature extraction model...\")\n",
    "full_model, feature_extractor = create_lightweight_feature_extractor((timesteps, n_features))\n",
    "\n",
    "# Compile with reduced learning rate\n",
    "full_model.compile(\n",
    "    optimizer=Adam(0.0005),  # Reduced learning rate\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"Training CNN+LSTM feature extractor (reduced epochs)...\")\n",
    "# Reduced training parameters\n",
    "history = full_model.fit(\n",
    "    X_train_reshaped, y_train_onehot,\n",
    "    validation_split=0.15,  # Smaller validation split\n",
    "    epochs=20,              # Reduced epochs\n",
    "    batch_size=64,          # Larger batch size\n",
    "    callbacks=[EarlyStopping(patience=5, restore_best_weights=True)],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Extract features\n",
    "print(\"Extracting features...\")\n",
    "X_train_features = feature_extractor.predict(X_train_reshaped, batch_size=128)\n",
    "X_test_features = feature_extractor.predict(X_test_reshaped, batch_size=128)\n",
    "\n",
    "# Memory cleanup\n",
    "del X_train_reshaped, X_test_reshaped, full_model\n",
    "gc.collect()\n",
    "\n",
    "print(f\"Original feature shape: {X_train_scaled.shape}\")\n",
    "print(f\"Extracted feature shape: {X_train_features.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926bc181",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T10:42:17.088906Z",
     "iopub.status.busy": "2025-06-27T10:42:17.088675Z",
     "iopub.status.idle": "2025-06-27T10:42:17.432949Z",
     "shell.execute_reply": "2025-06-27T10:42:17.432248Z",
     "shell.execute_reply.started": "2025-06-27T10:42:17.088890Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SIMPLIFIED DIMENSIONALITY REDUCTION: Use PCA instead of Kernel PCA\n",
    "# =============================================================================\n",
    "print(\"Applying PCA for dimensionality reduction...\")\n",
    "pca = PCA(n_components=16, random_state=42)  # Reduced components\n",
    "X_train_pca = pca.fit_transform(X_train_features)\n",
    "X_test_pca = pca.transform(X_test_features)\n",
    "\n",
    "print(f\"After PCA: {X_train_pca.shape}\")\n",
    "\n",
    "# Memory cleanup\n",
    "del X_train_features, X_test_features\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fba64b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T10:42:17.433847Z",
     "iopub.status.busy": "2025-06-27T10:42:17.433660Z",
     "iopub.status.idle": "2025-06-27T10:51:26.607292Z",
     "shell.execute_reply": "2025-06-27T10:51:26.606728Z",
     "shell.execute_reply.started": "2025-06-27T10:42:17.433831Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PHASE 1: Simplified KNN and Random Forest\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PHASE 1: Training KNN and Random Forest (Simplified)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Simplified classifiers\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=3)  # Reduced neighbors\n",
    "rf_classifier = RandomForestClassifier(n_estimators=50, max_depth=10, random_state=42)  # Reduced trees\n",
    "\n",
    "# Train classifiers\n",
    "print(\"Training simplified classifiers...\")\n",
    "knn_classifier.fit(X_train_pca, y_train_balanced)\n",
    "rf_classifier.fit(X_train_pca, y_train_balanced)\n",
    "\n",
    "# Get predictions and probabilities\n",
    "knn_proba_train = knn_classifier.predict_proba(X_train_pca)\n",
    "knn_proba_test = knn_classifier.predict_proba(X_test_pca)\n",
    "rf_proba_train = rf_classifier.predict_proba(X_train_pca)\n",
    "rf_proba_test = rf_classifier.predict_proba(X_test_pca)\n",
    "\n",
    "knn_pred_test = knn_classifier.predict(X_test_pca)\n",
    "rf_pred_test = rf_classifier.predict(X_test_pca)\n",
    "\n",
    "print(f\"KNN Test Accuracy: {accuracy_score(y_test, knn_pred_test):.4f}\")\n",
    "print(f\"Random Forest Test Accuracy: {accuracy_score(y_test, rf_pred_test):.4f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# FEATURE FUSION: Simplified\n",
    "# =============================================================================\n",
    "print(\"\\nPerforming Feature Fusion...\")\n",
    "\n",
    "# Simplified fusion - only probabilities\n",
    "train_fused_features = np.concatenate([\n",
    "    X_train_pca,\n",
    "    knn_proba_train,\n",
    "    rf_proba_train\n",
    "], axis=1)\n",
    "\n",
    "test_fused_features = np.concatenate([\n",
    "    X_test_pca,\n",
    "    knn_proba_test,\n",
    "    rf_proba_test\n",
    "], axis=1)\n",
    "\n",
    "print(f\"Fused feature shape: {train_fused_features.shape}\")\n",
    "\n",
    "# Memory cleanup\n",
    "del X_train_pca, X_test_pca, knn_proba_train, knn_proba_test, rf_proba_train, rf_proba_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff8cf13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T10:51:26.608482Z",
     "iopub.status.busy": "2025-06-27T10:51:26.608239Z",
     "iopub.status.idle": "2025-06-27T10:51:30.722153Z",
     "shell.execute_reply": "2025-06-27T10:51:30.719425Z",
     "shell.execute_reply.started": "2025-06-27T10:51:26.608461Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PHASE 2: Final Logistic Regression\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PHASE 2: Training Final Logistic Regression\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "lr_classifier = LogisticRegression(max_iter=500, random_state=42)  # Reduced iterations\n",
    "lr_classifier.fit(train_fused_features, y_train_balanced)\n",
    "\n",
    "final_pred_test = lr_classifier.predict(test_fused_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663479fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T10:51:30.724208Z",
     "iopub.status.busy": "2025-06-27T10:51:30.723935Z",
     "iopub.status.idle": "2025-06-27T10:51:48.502502Z",
     "shell.execute_reply": "2025-06-27T10:51:48.501877Z",
     "shell.execute_reply.started": "2025-06-27T10:51:30.724184Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SIMPLIFIED 3-FOLD CROSS VALIDATION (instead of 5-fold)\n",
    "# =============================================================================\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"5-FOLD CROSS VALIDATION - FINAL MODEL (Kaggle Optimized)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)  # Reduced to 3-fold\n",
    "cv_scores = []\n",
    "train_scores = []\n",
    "\n",
    "fold_num = 1\n",
    "for train_idx, val_idx in cv.split(train_fused_features, y_train_balanced):\n",
    "    print(f\"Processing Fold {fold_num}/5...\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train_fold = train_fused_features[train_idx]\n",
    "    X_val_fold = train_fused_features[val_idx]\n",
    "    y_train_fold = y_train_balanced.iloc[train_idx] if hasattr(y_train_balanced, 'iloc') else y_train_balanced[train_idx]\n",
    "    y_val_fold = y_train_balanced.iloc[val_idx] if hasattr(y_train_balanced, 'iloc') else y_train_balanced[val_idx]\n",
    "    \n",
    "    # Train classifier\n",
    "    lr_fold = LogisticRegression(max_iter=300, random_state=42)\n",
    "    lr_fold.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Evaluate\n",
    "    val_score = lr_fold.score(X_val_fold, y_val_fold)\n",
    "    train_score = lr_fold.score(X_train_fold, y_train_fold)\n",
    "    \n",
    "    cv_scores.append(val_score)\n",
    "    train_scores.append(train_score)\n",
    "    \n",
    "    print(f\"   Fold {fold_num} - Training: {train_score:.4f}, Testing: {val_score:.4f}\")\n",
    "    fold_num += 1\n",
    "\n",
    "cv_scores = np.array(cv_scores)\n",
    "train_scores = np.array(train_scores)\n",
    "\n",
    "print(f\"\\nCross-validation scores: {cv_scores}\")\n",
    "print(f\"Mean validation accuracy: {np.mean(cv_scores):.4f}\")\n",
    "print(f\"Standard deviation: {np.std(cv_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d007d0f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T10:51:48.503363Z",
     "iopub.status.busy": "2025-06-27T10:51:48.503184Z",
     "iopub.status.idle": "2025-06-27T10:51:49.532250Z",
     "shell.execute_reply": "2025-06-27T10:51:49.531571Z",
     "shell.execute_reply.started": "2025-06-27T10:51:48.503349Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# RESULTS AND VISUALIZATION\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Final Test Accuracy: {accuracy_score(y_test, final_pred_test):.4f}\")\n",
    "print(f\"Cross-Validation Mean: {np.mean(cv_scores):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "# Get the classification report as a dictionary\n",
    "report_dict = classification_report(y_test, final_pred_test, \n",
    "                                  target_names=list(attack_mapping.keys()),\n",
    "                                  output_dict=True)\n",
    "\n",
    "# Custom formatting function to display percentages\n",
    "def format_percentage(value):\n",
    "    return f\"{value * 100:.2f}\"\n",
    "\n",
    "# Print formatted classification report\n",
    "print(f\"{'':20} {'precision':>10} {'recall':>10} {'f1-score':>10} {'support':>10}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for class_name in list(attack_mapping.keys()):\n",
    "    if class_name in report_dict:\n",
    "        precision = format_percentage(report_dict[class_name]['precision'])\n",
    "        recall = format_percentage(report_dict[class_name]['recall'])\n",
    "        f1_score = format_percentage(report_dict[class_name]['f1-score'])\n",
    "        support = report_dict[class_name]['support']\n",
    "        print(f\"{class_name:20} {precision:>10} {recall:>10} {f1_score:>10} {support:>10.0f}\")\n",
    "\n",
    "print(\"-\" * 65)\n",
    "# Print accuracy, macro avg, and weighted avg\n",
    "accuracy = format_percentage(report_dict['accuracy'])\n",
    "macro_precision = format_percentage(report_dict['macro avg']['precision'])\n",
    "macro_recall = format_percentage(report_dict['macro avg']['recall'])\n",
    "macro_f1 = format_percentage(report_dict['macro avg']['f1-score'])\n",
    "macro_support = report_dict['macro avg']['support']\n",
    "\n",
    "weighted_precision = format_percentage(report_dict['weighted avg']['precision'])\n",
    "weighted_recall = format_percentage(report_dict['weighted avg']['recall'])\n",
    "weighted_f1 = format_percentage(report_dict['weighted avg']['f1-score'])\n",
    "weighted_support = report_dict['weighted avg']['support']\n",
    "\n",
    "print(f\"{'accuracy':20} {'':<10} {'':<10} {accuracy:>10} {macro_support:>10.0f}\")\n",
    "print(f\"{'macro avg':20} {macro_precision:>10} {macro_recall:>10} {macro_f1:>10} {macro_support:>10.0f}\")\n",
    "print(f\"{'weighted avg':20} {weighted_precision:>10} {weighted_recall:>10} {weighted_f1:>10} {weighted_support:>10.0f}\")\n",
    "\n",
    "# Simplified visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, 6), cv_scores, marker='o', label='Testing Accuracy', \n",
    "         color='blue', linestyle='-', linewidth=2, markersize=8)\n",
    "plt.plot(range(1, 6), train_scores, marker='x', label='Training Accuracy', \n",
    "         color='red', linestyle='--', linewidth=2, markersize=10)\n",
    "\n",
    "# Add labels for validation accuracy points\n",
    "for i, score in enumerate(cv_scores):\n",
    "    plt.annotate(f'{score:.4f}', (i+1, score), textcoords=\"offset points\", \n",
    "                xytext=(5,5), ha='left', fontsize=9, color='blue')\n",
    "\n",
    "# Add labels for training accuracy points\n",
    "for i, score in enumerate(train_scores):\n",
    "    plt.annotate(f'{score:.4f}', (i+1, score), textcoords=\"offset points\", \n",
    "                xytext=(5,-15), ha='left', fontsize=9, color='red')\n",
    "\n",
    "plt.title('5-Fold Cross-Validation Results (R-NIDS)', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Fold Number')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Confusion Matrix with Percentages (2 decimal places)\n",
    "plt.figure(figsize=(10, 8))\n",
    "cm = confusion_matrix(y_test, final_pred_test)\n",
    "# Convert to percentages\n",
    "cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "# Create the heatmap with percentages (2 decimal places)\n",
    "sns.heatmap(cm_percentage, annot=True, fmt='.2f', cmap='Blues', \n",
    "            xticklabels=list(attack_mapping.keys()),\n",
    "            yticklabels=list(attack_mapping.keys()),\n",
    "            cbar_kws={'label': 'Percentage (%)'})\n",
    "plt.title('Confusion Matrix (Percentages)\\nR-NIDS', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Optional: Also show raw counts confusion matrix for reference\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', \n",
    "            xticklabels=list(attack_mapping.keys()),\n",
    "            yticklabels=list(attack_mapping.keys()),\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.title('Confusion Matrix (Raw Counts)\\nR-NIDS', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"KAGGLE-OPTIMIZED PIPELINE COMPLETED!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7324106,
     "sourceId": 11670509,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 22.751501,
   "end_time": "2025-07-11T13:09:40.239994",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-11T13:09:17.488493",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
