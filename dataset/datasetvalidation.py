# -*- coding: utf-8 -*-
"""datasetvalidation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KlgvYsOQAYr03p89JlWbW2sbENcDR25t
"""

from joblib import load
import pandas as pd

# Load model, scaler, and features
model = load("model.joblib")
scaler = load("scaler.joblib")   # skip if you didn’t save scaler

with open("features.txt") as f:
    features = [line.strip() for line in f]

print("Features order:", features)

print("Model expects:", features[:10], "...")  # first 10 expected features
print("CSV has:", list(new_data.columns)[:10], "...")  # first 10 columns from CSV

missing = set(features) - set(new_data.columns)
extra = set(new_data.columns) - set(features)

print("Missing in CSV:", missing)
print("Extra in CSV:", extra)

# Force reordering of columns to match training order
new_data = new_data.reindex(columns=features)

# Keep only model's features (drop all others)
new_data = new_data.loc[:, new_data.columns.intersection(features)]

# Reorder columns to exactly match training
new_data = new_data.reindex(columns=features)

print("Expected features:", features)
print("New data columns :", list(new_data.columns))
print("Shape of new_data:", new_data.shape)

new_data = new_data[features].copy()

# Clean up column names (remove hidden spaces etc.)
new_data.columns = new_data.columns.str.strip()

# Now force subset to ONLY training features
new_data = new_data.loc[:, features]

# Confirm
print("Training features:", features)
print("New data columns:", list(new_data.columns))
print("Same?", features == list(new_data.columns))

print("Scaler feature names:", list(scaler.feature_names_in_))
print("New data columns    :", list(new_data.columns))
print("Match?", list(scaler.feature_names_in_) == list(new_data.columns))

new_data = new_data.loc[:, scaler.feature_names_in_]

import pandas as pd
import numpy as np
from joblib import load

# === Load model + scaler ===
model = load("model.joblib")
scaler = load("scaler.joblib")

# Expected features from scaler
expected_features = list(scaler.feature_names_in_)

def prepare_and_predict(csv_file):
    # 1. Load CSV
    df = pd.read_csv(csv_file)

    # 2. Keep only required columns (drop timestamps or extras)
    missing = set(expected_features) - set(df.columns)
    extra = set(df.columns) - set(expected_features)
    print("Missing in CSV:", missing)
    print("Extra in CSV:", extra)

    df = df.loc[:, expected_features]  # keep only what we need, in correct order

    # 3. Clean data
    df = df.replace([np.inf, -np.inf], np.nan)  # replace inf with NaN
    df = df.fillna(0)  # fill NaN with 0 (safe default, you can change strategy)

    # 4. Scale
    X_scaled = scaler.transform(df)

    # 5. Predict
    preds = model.predict(X_scaled)
    return preds

# === Run prediction on your new CSV ===
predictions = prepare_and_predict("/content/normalcompiled.pcap_Flow.csv") # replace data here <-----------------------
print(predictions)

def prepare_and_predict(csv_file, save_cleaned=False):
    df = pd.read_csv(csv_file)
    df = df.loc[:, expected_features]
    df = df.replace([np.inf, -np.inf], np.nan).fillna(0)

    if save_cleaned:
        df.to_csv("cleaned_for_model.csv", index=False)
        print("Saved cleaned CSV as cleaned_for_model.csv")

    X_scaled = scaler.transform(df)
    return model.predict(X_scaled)

predictions = prepare_and_predict("/content/normalcompiled.pcap_Flow.csv", save_cleaned=True)

def prepare_and_predict(csv_file, save_cleaned=False):
    df = pd.read_csv(csv_file)
    df = df.loc[:, expected_features]
    df = df.replace([np.inf, -np.inf], np.nan).fillna(0)

    if save_cleaned:
        df.to_csv("cleaned_for_model.csv", index=False)
        print("Saved cleaned CSV as cleaned_for_model.csv")

    X_scaled = scaler.transform(df)
    preds = model.predict(X_scaled)

    # Convert numeric predictions into labels
    decoded_preds = [label_map.get(p, f"Unknown({p})") for p in preds]
    return decoded_preds

# Define label mapping (example — adjust if your classes differ!)
label_map = {
    0: "BENIGN",
    1: "Botnet",
    2: "Brute Force",
    3: "DDoS",
    4: "DoS",
    5: "Infiltration",
    6: "PortScan",
    7: "Web Attack"
}

def prepare_and_predict(csv_file, save_cleaned=False):
    df = pd.read_csv(csv_file)
    df = df.loc[:, expected_features]
    df = df.replace([np.inf, -np.inf], np.nan).fillna(0)

    if save_cleaned:
        df.to_csv("cleaned_for_model.csv", index=False)
        print("Saved cleaned CSV as cleaned_for_model.csv")

    X_scaled = scaler.transform(df)
    preds = model.predict(X_scaled)

    # Convert numeric predictions into labels
    decoded_preds = [label_map.get(p, f"Unknown({p})") for p in preds]
    return decoded_preds

predictions = prepare_and_predict("/content/normalcompiled.pcap_Flow.csv", save_cleaned=True)

# Print first 10 results
print(predictions[:10])

# Count benign vs malicious
from collections import Counter
counts = Counter(predictions)

print("\nSummary of predictions:")
print(counts)

if counts["BENIGN"] == len(predictions):
    print("✅ All traffic is benign.")
elif counts["BENIGN"] == 0:
    print("⚠️ All traffic is malicious.")
else:
    print(f"Mix of benign and malicious: {counts['BENIGN']} benign, {len(predictions)-counts['BENIGN']} malicious")

predictions = prepare_and_predict("/content/path2normal1.pcap_Flow.csv", save_cleaned=True)

# Print first 10 results
print(predictions[:10])

# Count benign vs malicious
from collections import Counter
counts = Counter(predictions)

print("\nSummary of predictions:")
print(counts)

if counts["BENIGN"] == len(predictions):
    print("✅ All traffic is benign.")
elif counts["BENIGN"] == 0:
    print("⚠️ All traffic is malicious.")
else:
    print(f"Mix of benign and malicious: {counts['BENIGN']} benign, {len(predictions)-counts['BENIGN']} malicious")